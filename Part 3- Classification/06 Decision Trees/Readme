Decision Tree:
The core algorithm for building decision trees called ID3 which employs a top-down, greedy search through the space of possible branches with no backtracking. 
ID3 uses Entropy and Information Gain to construct a decision tree.

In most of the cases, complete splits of the data doesn't occur.
In that case, at the terminal node, a probabilistic classification occurs.(ie.Likelihood of the which class is max at the node--- in other words, mode)

Additional methods on top of Decision Trees:
1. RF
2. Gradient boosting 

Criterions used: http://www.saedsayad.com/decision_tree.htm
1. Gini impurity(lowet gini impurity is chosen): 1 – ( P(class1)^2 + P(class2)^2 + … + P(classN)^2)
                 Favors larger partitions.
                 Uses squared proportion of classes.
                 Perfectly classified, Gini Index would be zero.
                 Evenly distributed would be 1 – (1/# Classes).
                 You want a variable split that has a low Gini Index.
The algorithm works as 1 – ( P(class1)^2 + P(class2)^2 + … + P(classN)^2)
2. Information gain(highest information gain..ie.lowest entropy is chosen): Entropy pf parent- weighted avg of the entropy of the children if you split the parent
                        Entropy= sum(-prob.log base2(prob))
                        Entropy of 2 attributes(or categories)= sum(prob.of attribute*Entropy of the attribute)
                        IG of leaf node = 0 and Entropy of equally divided sample =1 

Differences:
1. It only matters in 2% of the cases whether you use gini impurity or entropy.
2. Entropy might be a little slower to compute (because it makes use of the logarithm).
